{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Richter's Predictor\n",
    "\n",
    "Initial code is a copy of the example found here: http://drivendata.co/blog/richters-predictor-benchmark/\n",
    "\n",
    "We'll then use an XGBoost model to get a better estimate, and also will look at engineering some features using the geocode.\n",
    "\n",
    "The intention is to try to find a way to use the fact that some areas (geo locations) will have suffered more damage than others.  There seem to be too many level 3 geolocations for a tree-based algorithm to deal with effectively, but if we can somehow uncover information about the geolocation and encode it in a way that is easier for the tree to deal with then it may improve our scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Andrew/anaconda/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# for preprocessing the data\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# the model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "# for combining the preprocess with model training\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# for optimizing the hyperparameters of the pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('.', 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_values = pd.read_csv(DATA_DIR / 'train_values.csv', index_col='building_id')\n",
    "train_labels = pd.read_csv(DATA_DIR / 'train_labels.csv', index_col='building_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_values  = pd.read_csv(DATA_DIR / 'test_values.csv', index_col='building_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For some feature engineering we want to use the damage grades, so we'll join them here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_values.join(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For others we also want to include test values (eg. when encoding categorical) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_and_train_values = pd.concat([train_values,test_values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_values.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_and_train_values.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_labels.damage_grade\n",
    "             .value_counts()\n",
    "             .sort_index()\n",
    "             .plot.bar(title=\"Number of Buildings with Each Damage Grade\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['foundation_type', \n",
    "                     'area_percentage', \n",
    "                     'height_percentage',\n",
    "                     'count_floors_pre_eq',\n",
    "                     'land_surface_condition',\n",
    "                     'has_superstructure_cement_mortar_stone']\n",
    "\n",
    "train_values_subset = train_values[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(train_values_subset.join(train_labels), \n",
    "             hue='damage_grade')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondary_uses = [\n",
    "'has_secondary_use',\n",
    "'has_secondary_use_agriculture',\n",
    "'has_secondary_use_hotel',\n",
    "'has_secondary_use_rental',\n",
    "'has_secondary_use_institution',\n",
    "'has_secondary_use_school',\n",
    "'has_secondary_use_industry',\n",
    "'has_secondary_use_health_post',\n",
    "'has_secondary_use_gov_office',\n",
    "'has_secondary_use_use_police',\n",
    "'has_secondary_use_other'\n",
    "]\n",
    "\n",
    "structure = [\n",
    "'has_superstructure_adobe_mud',\n",
    "'has_superstructure_mud_mortar_stone',\n",
    "'has_superstructure_stone_flag',\n",
    "'has_superstructure_cement_mortar_stone',\n",
    "'has_superstructure_mud_mortar_brick',\n",
    "'has_superstructure_cement_mortar_brick',\n",
    "'has_superstructure_timber',\n",
    "'has_superstructure_bamboo',\n",
    "'has_superstructure_rc_non_engineered',\n",
    "'has_superstructure_rc_engineered',\n",
    "'has_superstructure_other'\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for use in secondary_uses:\n",
    "    print(use, train[train[use]==1]['damage_grade'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in structure:\n",
    "    print(s, train[train[s]==1]['damage_grade'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biggest_geo3 = train['geo_level_3_id'].value_counts().head(30).index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for location in biggest_geo3:\n",
    "    print('Geo3 id:',location)\n",
    "    for s in structure:\n",
    "        s_filter = (train['geo_level_3_id'] == location) & (train[s] == 1)\n",
    "        print(s, train.loc[s_filter]['damage_grade'].count(), train.loc[s_filter]['damage_grade'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are similarities in damage between the mortar types (mud/cement) and the reinforced concrete types (non-eng, engineered) so for the sake of our geoid indicator we'll group them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train['mud'] = train['has_superstructure_adobe_mud'] | train['has_superstructure_mud_mortar_stone'] | train['has_superstructure_mud_mortar_brick']\n",
    "train['cement'] = train['has_superstructure_cement_mortar_stone'] | train['has_superstructure_cement_mortar_brick'] \n",
    "train['concrete'] = train['has_superstructure_rc_non_engineered'] | train['has_superstructure_rc_engineered'] \n",
    "train['natural'] = train['has_superstructure_timber'] | train['has_superstructure_bamboo'] \n",
    "\n",
    "test_and_train_values['mud'] = test_and_train_values['has_superstructure_adobe_mud'] | test_and_train_values['has_superstructure_mud_mortar_stone'] | test_and_train_values['has_superstructure_mud_mortar_brick']\n",
    "test_and_train_values['cement'] = test_and_train_values['has_superstructure_cement_mortar_stone'] | test_and_train_values['has_superstructure_cement_mortar_brick'] \n",
    "test_and_train_values['concrete'] = test_and_train_values['has_superstructure_rc_non_engineered'] | test_and_train_values['has_superstructure_rc_engineered'] \n",
    "test_and_train_values['natural'] = test_and_train_values['has_superstructure_timber'] | test_and_train_values['has_superstructure_bamboo'] \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some geolocations might only be in the test set, so if we are going to build a universal lookup then we need to include test as well so we can get a complete list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_and_train_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_and_train.head()\n",
    "test_and_train_values.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_lookup = test_and_train_values[['geo_level_3_id','geo_level_2_id','geo_level_1_id']].groupby(['geo_level_3_id']).first().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "geo_lookup.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OK, now we can calcualte averages for each geoid level and construction type, and build our lookup table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I'm going to double check what the frequency is like for the different structure types, because I'm worried that we won't have many examples of concrete and if that is an issue then we need to account for it somehow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "f, axes = plt.subplots(1, 4, figsize=(14, 4), sharex=True, sharey=True)\n",
    "\n",
    "# Change the x-axis because it has a really long tail.  First attempt was to make it log, second just trims\n",
    "#axes[0,0].set(xscale=\"log\")\n",
    "axes[0].set_xlim(right=20)\n",
    "\n",
    "graph_colours = ['skyblue','olive', 'gold', 'teal']\n",
    "ax = [axes[0],axes[1],axes[2],axes[3]]\n",
    "\n",
    "for i,s in enumerate(structure_cats):\n",
    "    sns.distplot( geo_lookup[s+'1_n'].fillna(0) , color=graph_colours[i], ax=ax[i], bins=300)\n",
    "\n",
    "#################################\n",
    "f, axes = plt.subplots(1, 4, figsize=(14, 4), sharex=True, sharey=True)\n",
    "\n",
    "# Change the x-axis because it has a really long tail.  First attempt was to make it log, second just trims\n",
    "#axes[0,0].set(xscale=\"log\")\n",
    "axes[0].set_xlim(right=20)\n",
    "\n",
    "graph_colours = ['skyblue','olive', 'gold', 'teal']\n",
    "ax = [axes[0],axes[1],axes[2],axes[3]]\n",
    "\n",
    "for i,s in enumerate(structure_cats):\n",
    "    sns.distplot( geo_lookup[s+'2_n'].fillna(0) , color=graph_colours[i], ax=ax[i], bins=300)\n",
    "\n",
    "\n",
    "#################################\n",
    "f, axes = plt.subplots(2, 2, figsize=(7, 7), sharex=True, sharey=True)\n",
    "\n",
    "# Change the x-axis because it has a really long tail.  First attempt was to make it log, second just trims\n",
    "#axes[0,0].set(xscale=\"log\")\n",
    "axes[0,0].set_xlim(right=20)\n",
    "\n",
    "graph_colours = ['skyblue','olive', 'gold', 'teal']\n",
    "ax = [axes[0, 0],axes[0, 1],axes[1, 0],axes[1, 1]]\n",
    "\n",
    "\n",
    "for i,s in enumerate(structure_cats):\n",
    "    sns.distplot( geo_lookup[s+'3_n'].fillna(0) , color=graph_colours[i], ax=ax[i], bins=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure_cats = ['mud','natural', 'cement', 'concrete']\n",
    "\n",
    "for location in biggest_geo3:\n",
    "    print('Geo3 id:',location)\n",
    "    for s in structure_cats:\n",
    "        s_filter = (train['geo_level_3_id'] == location) & (train[s] == 1)\n",
    "        print(s, train.loc[s_filter]['damage_grade'].count(), train.loc[s_filter]['damage_grade'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages = {}\n",
    "levels = ['1','2','3']\n",
    "for level in levels:\n",
    "    for s in structure_cats:\n",
    "        s_filter = train[s] == 1\n",
    "        averages[s+level] = train[s_filter].groupby('geo_level_'+level+'_id')['damage_grade'].agg({s+level+'_n':'count', \n",
    "                                     s+level+'_mean':'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages['mud1'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.merge(averages['mud2'].reset_index(), how='left',on='geo_level_2_id').head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Everything is looking OK, let's do it again but this time merge inline rather than saving to a dictionary first\n",
    "We use train here because that df has got the damage values in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "averaging level 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Andrew/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:13: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "averaging level 2\n",
      "averaging level 3\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "levels = ['1','2','3']\n",
    "structure_cats = ['mud','natural', 'cement', 'concrete']\n",
    "\n",
    "\n",
    "for level in levels:\n",
    "    print('averaging level',level)\n",
    "    averages_list = []\n",
    "\n",
    "    # Work out normalised damage grades for each structure type\n",
    "    for s in structure_cats:\n",
    "        s_filter = train[s] == 1\n",
    "        averages = train[s_filter].groupby('geo_level_'+level+'_id')['damage_grade'].agg({s+level+'_n':'count', \n",
    "                                     s+level+'_mean':'mean'})\n",
    "        col_to_norm = averages[s+level+'_mean']\n",
    "        averages[s+level+'_mean_norm']=(col_to_norm-col_to_norm.min())/(col_to_norm.max()-col_to_norm.min())\n",
    "        #print(averages.head(2))\n",
    "        averages_list.append(averages)\n",
    "\n",
    "    # Concat the averages into one dataframe\n",
    "    averages = pd.concat(averages_list, axis=1)\n",
    "    #print(averages.head())\n",
    "    #print(geo_lookup.shape)\n",
    "    \n",
    "    # Now we have those, we can also calculate a weighted avergage across the structure types for that geoid\n",
    "    \n",
    "    cols = [s+level+'_mean_norm' for s in structure_cats]\n",
    "    weights = [s+level+'_n' for s in structure_cats]\n",
    "\n",
    "    norms_np = averages[cols].values\n",
    "    weights_np = averages[weights].values\n",
    "\n",
    "    norm_mask = np.isnan(norms_np)\n",
    "    weights_mask = np.isnan(weights_np)\n",
    "\n",
    "    norms_np = np.ma.masked_array(norms_np, mask=norm_mask)\n",
    "    weights_np = np.ma.masked_array(weights_np, mask=weights_mask)\n",
    "\n",
    "    wa_norm = np.ma.average(norms_np, weights=weights_np, axis=1)\n",
    "    wa_norm.fill_value = -1\n",
    "    averages['level'+level+'norm_damage'] = wa_norm.filled()\n",
    "    #geo_lookup['level'+level+'_wa_norm_damage'] = wa_norm.filled()\n",
    "    #print(averages.head())\n",
    "    geo_lookup = geo_lookup.merge(averages['level'+level+'norm_damage'].reset_index(), how='left',on='geo_level_'+level+'_id')\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some  geoids have missing values though, presumably because the only examples are in the training set.  So we can use the next available level up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_lookup.loc[8313]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_level = geo_lookup['level2norm_damage'].isnull()\n",
    "geo_lookup.loc[empty_level,'level2norm_damage'] = geo_lookup.loc[empty_level,'level1norm_damage']\n",
    "\n",
    "empty_level = geo_lookup['level3norm_damage'].isnull()\n",
    "geo_lookup.loc[empty_level,'level3norm_damage'] = geo_lookup.loc[empty_level,'level2norm_damage']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_lookup.loc[8313]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, join the lookup table with the test/train values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_and_train_values = test_and_train_values.merge(geo_lookup[['geo_level_3_id','level3norm_damage']], on=['geo_level_3_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_and_train_values[22011:22015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_and_train_values.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_values.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the categorical into numeric\n",
    "\n",
    "This code taken from https://stackoverflow.com/questions/24458645/label-encoding-across-multiple-columns-in-scikit-learn\n",
    "\n",
    "categorical_encoder = defaultdict(LabelEncoder)\n",
    "\n",
    "'# Encoding the variable\n",
    "\n",
    "fit = df.apply(lambda x: categorical_encoder[x.name].fit_transform(x))\n",
    "\n",
    "'# Inverse the encoded\n",
    "\n",
    "fit.apply(lambda x: categorical_encoder[x.name].inverse_transform(x))\n",
    "\n",
    "'# Using the dictionary to label future data\n",
    "\n",
    "df.apply(lambda x: categorical_encoder[x.name].transform(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\n",
    "    'land_surface_condition',\n",
    "    'foundation_type',\n",
    "    'roof_type',\n",
    "    'ground_floor_type',\n",
    "    'other_floor_type',\n",
    "    'position',\n",
    "    'plan_configuration',\n",
    "    'legal_ownership_status'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_encoder = defaultdict(LabelEncoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the encoder on the combined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_and_train_values.loc[:,categorical_columns] = test_and_train_values.loc[:,categorical_columns].apply(lambda x: categorical_encoder[x.name].fit_transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_and_train_values.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the encoder to transform the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_values.loc[:,categorical_columns] = train_values.loc[:,categorical_columns].apply(lambda x: categorical_encoder[x.name].transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_values.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All features are now categorical so we could use them on a classifier now, but...let's engineer some features first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(geo_lookup[['geo_level_3_id','level3norm_damage']], on=['geo_level_3_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[:,categorical_columns] = train.loc[:,categorical_columns].apply(lambda x: categorical_encoder[x.name].transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Damage measures for each geolocation\n",
    "\n",
    "The idae here is that I want a normalised measure of average damage per geolocation.  The easiest way would be to take the average damage value for the geoid but that wouldn't take into consideration the different mix of building types.  If some geolocations had sturdier buildings then it's average damage might be artifically low.\n",
    "\n",
    "So instead I'll take the average damage for each building type and/or some sort of adjustment for the building type - for example wooden buildings seem to have less damage so perhaps we can work out some normalised values for each building type and then combine them to get a single normalised damage value for each geoid.\n",
    "\n",
    "The last factor I want to allow for is that some geoids have only one data point, and that isn't gong to be of much use, so instead my intial approach will be to take the average of the next geolevel up if the count of datapoints is below a certain threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ex_geo = train.drop(['geo_level_1_id','geo_level_2_id','geo_level_3_id', 'damage_grade'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_ex_geo, train['damage_grade'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=500, max_depth=10)\n",
    "clf.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "f1_score(y_test, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OK, now let's try that with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train-1)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify parameters via map\n",
    "param = {'max_depth':10, 'eta':1, 'objective':'multi:softmax', 'num_class':3 }\n",
    "num_round = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst = xgb.train(param, dtrain, num_round)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction\n",
    "y_pred = bst.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7450164041365284"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred+1, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
