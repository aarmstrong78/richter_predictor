{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Richter's Predictor\n",
    "\n",
    "Initial code is a copy of the example found here: http://drivendata.co/blog/richters-predictor-benchmark/\n",
    "\n",
    "We'll then use an XGBoost model to get a better estimate, and also will look at engineering some features using the geocode.\n",
    "\n",
    "The intention is to try to find a way to use the fact that some areas (geo locations) will have suffered more damage than others.  There seem to be too many level 3 geolocations for a tree-based algorithm to deal with effectively, but if we can somehow uncover information about the geolocation and encode it in a way that is easier for the tree to deal with then it may improve our scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Andrew/anaconda/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# for preprocessing the data\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# the model\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score, make_scorer\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# for combining the preprocess with model training\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# for optimizing the hyperparameters of the pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import itertools\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('.', 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_values = pd.read_csv(DATA_DIR / 'train_values.csv', index_col='building_id')\n",
    "train_labels = pd.read_csv(DATA_DIR / 'train_labels.csv', index_col='building_id')\n",
    "test_values  = pd.read_csv(DATA_DIR / 'test_values.csv', index_col='building_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = train_values.index\n",
    "test_index = test_values.index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We need to make sure we engineer features comparably across the train and test sets, so lets join them up here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_and_train_values = pd.concat([train_values,test_values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86868, 38)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_and_train_values.loc[test_index].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For some feature engineering we want to use the damage grades, so we'll join them here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_and_train = test_and_train_values.join(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geo_level_1_id</th>\n",
       "      <th>geo_level_2_id</th>\n",
       "      <th>geo_level_3_id</th>\n",
       "      <th>count_floors_pre_eq</th>\n",
       "      <th>age</th>\n",
       "      <th>area_percentage</th>\n",
       "      <th>height_percentage</th>\n",
       "      <th>land_surface_condition</th>\n",
       "      <th>foundation_type</th>\n",
       "      <th>roof_type</th>\n",
       "      <th>...</th>\n",
       "      <th>has_secondary_use_hotel</th>\n",
       "      <th>has_secondary_use_rental</th>\n",
       "      <th>has_secondary_use_institution</th>\n",
       "      <th>has_secondary_use_school</th>\n",
       "      <th>has_secondary_use_industry</th>\n",
       "      <th>has_secondary_use_health_post</th>\n",
       "      <th>has_secondary_use_gov_office</th>\n",
       "      <th>has_secondary_use_use_police</th>\n",
       "      <th>has_secondary_use_other</th>\n",
       "      <th>damage_grade</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>building_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>300051</th>\n",
       "      <td>17</td>\n",
       "      <td>596</td>\n",
       "      <td>11307</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99355</th>\n",
       "      <td>6</td>\n",
       "      <td>141</td>\n",
       "      <td>11987</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890251</th>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>10044</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745817</th>\n",
       "      <td>26</td>\n",
       "      <td>39</td>\n",
       "      <td>633</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>x</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421793</th>\n",
       "      <td>17</td>\n",
       "      <td>289</td>\n",
       "      <td>7970</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>t</td>\n",
       "      <td>r</td>\n",
       "      <td>q</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             geo_level_1_id  geo_level_2_id  geo_level_3_id  \\\n",
       "building_id                                                   \n",
       "300051                   17             596           11307   \n",
       "99355                     6             141           11987   \n",
       "890251                   22              19           10044   \n",
       "745817                   26              39             633   \n",
       "421793                   17             289            7970   \n",
       "\n",
       "             count_floors_pre_eq  age  area_percentage  height_percentage  \\\n",
       "building_id                                                                 \n",
       "300051                         3   20                7                  6   \n",
       "99355                          2   25               13                  5   \n",
       "890251                         2    5                4                  5   \n",
       "745817                         1    0               19                  3   \n",
       "421793                         3   15                8                  7   \n",
       "\n",
       "            land_surface_condition foundation_type roof_type      ...       \\\n",
       "building_id                                                       ...        \n",
       "300051                           t               r         n      ...        \n",
       "99355                            t               r         n      ...        \n",
       "890251                           t               r         n      ...        \n",
       "745817                           t               r         x      ...        \n",
       "421793                           t               r         q      ...        \n",
       "\n",
       "            has_secondary_use_hotel has_secondary_use_rental  \\\n",
       "building_id                                                    \n",
       "300051                            0                        0   \n",
       "99355                             0                        0   \n",
       "890251                            0                        0   \n",
       "745817                            0                        1   \n",
       "421793                            0                        0   \n",
       "\n",
       "            has_secondary_use_institution has_secondary_use_school  \\\n",
       "building_id                                                          \n",
       "300051                                  0                        0   \n",
       "99355                                   0                        0   \n",
       "890251                                  0                        0   \n",
       "745817                                  0                        0   \n",
       "421793                                  0                        0   \n",
       "\n",
       "             has_secondary_use_industry  has_secondary_use_health_post  \\\n",
       "building_id                                                              \n",
       "300051                                0                              0   \n",
       "99355                                 0                              0   \n",
       "890251                                0                              0   \n",
       "745817                                0                              0   \n",
       "421793                                0                              0   \n",
       "\n",
       "             has_secondary_use_gov_office  has_secondary_use_use_police  \\\n",
       "building_id                                                               \n",
       "300051                                  0                             0   \n",
       "99355                                   0                             0   \n",
       "890251                                  0                             0   \n",
       "745817                                  0                             0   \n",
       "421793                                  0                             0   \n",
       "\n",
       "             has_secondary_use_other  damage_grade  \n",
       "building_id                                         \n",
       "300051                             0           NaN  \n",
       "99355                              0           NaN  \n",
       "890251                             0           NaN  \n",
       "745817                             0           NaN  \n",
       "421793                             0           NaN  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_and_train.loc[test_index].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "geo_level_1_id                              int64\n",
       "geo_level_2_id                              int64\n",
       "geo_level_3_id                              int64\n",
       "count_floors_pre_eq                         int64\n",
       "age                                         int64\n",
       "area_percentage                             int64\n",
       "height_percentage                           int64\n",
       "land_surface_condition                     object\n",
       "foundation_type                            object\n",
       "roof_type                                  object\n",
       "ground_floor_type                          object\n",
       "other_floor_type                           object\n",
       "position                                   object\n",
       "plan_configuration                         object\n",
       "has_superstructure_adobe_mud                int64\n",
       "has_superstructure_mud_mortar_stone         int64\n",
       "has_superstructure_stone_flag               int64\n",
       "has_superstructure_cement_mortar_stone      int64\n",
       "has_superstructure_mud_mortar_brick         int64\n",
       "has_superstructure_cement_mortar_brick      int64\n",
       "has_superstructure_timber                   int64\n",
       "has_superstructure_bamboo                   int64\n",
       "has_superstructure_rc_non_engineered        int64\n",
       "has_superstructure_rc_engineered            int64\n",
       "has_superstructure_other                    int64\n",
       "legal_ownership_status                     object\n",
       "count_families                              int64\n",
       "has_secondary_use                           int64\n",
       "has_secondary_use_agriculture               int64\n",
       "has_secondary_use_hotel                     int64\n",
       "has_secondary_use_rental                    int64\n",
       "has_secondary_use_institution               int64\n",
       "has_secondary_use_school                    int64\n",
       "has_secondary_use_industry                  int64\n",
       "has_secondary_use_health_post               int64\n",
       "has_secondary_use_gov_office                int64\n",
       "has_secondary_use_use_police                int64\n",
       "has_secondary_use_other                     int64\n",
       "damage_grade                              float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_and_train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x10cb3fcc0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x124071fd0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(train_labels.damage_grade\n",
    "             .value_counts()\n",
    "             .sort_index()\n",
    "             .plot.bar(title=\"Number of Buildings with Each Damage Grade\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['foundation_type', \n",
    "                     'area_percentage', \n",
    "                     'height_percentage',\n",
    "                     'count_floors_pre_eq',\n",
    "                     'land_surface_condition',\n",
    "                     'has_superstructure_cement_mortar_stone']\n",
    "\n",
    "train_values_subset = train_values[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(train_values_subset.join(train_labels), \n",
    "             hue='damage_grade')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondary_uses = [\n",
    "'has_secondary_use',\n",
    "'has_secondary_use_agriculture',\n",
    "'has_secondary_use_hotel',\n",
    "'has_secondary_use_rental',\n",
    "'has_secondary_use_institution',\n",
    "'has_secondary_use_school',\n",
    "'has_secondary_use_industry',\n",
    "'has_secondary_use_health_post',\n",
    "'has_secondary_use_gov_office',\n",
    "'has_secondary_use_use_police',\n",
    "'has_secondary_use_other'\n",
    "]\n",
    "\n",
    "structure = [\n",
    "'has_superstructure_adobe_mud',\n",
    "'has_superstructure_mud_mortar_stone',\n",
    "'has_superstructure_stone_flag',\n",
    "'has_superstructure_cement_mortar_stone',\n",
    "'has_superstructure_mud_mortar_brick',\n",
    "'has_superstructure_cement_mortar_brick',\n",
    "'has_superstructure_timber',\n",
    "'has_superstructure_bamboo',\n",
    "'has_superstructure_rc_non_engineered',\n",
    "'has_superstructure_rc_engineered',\n",
    "'has_superstructure_other'\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for use in secondary_uses:\n",
    "    print(use, train[train[use]==1]['damage_grade'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in structure:\n",
    "    print(s, train[train[s]==1]['damage_grade'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biggest_geo3 = train['geo_level_3_id'].value_counts().head(30).index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for location in biggest_geo3:\n",
    "    print('Geo3 id:',location)\n",
    "    for s in structure:\n",
    "        s_filter = (train['geo_level_3_id'] == location) & (train[s] == 1)\n",
    "        print(s, train.loc[s_filter]['damage_grade'].count(), train.loc[s_filter]['damage_grade'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are similarities in damage between the mortar types (mud/cement) and the reinforced concrete types (non-eng, engineered) so for the sake of our geoid indicator we'll group them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train['mud'] = train['has_superstructure_adobe_mud'] | train['has_superstructure_mud_mortar_stone'] | train['has_superstructure_mud_mortar_brick']\n",
    "train['cement'] = train['has_superstructure_cement_mortar_stone'] | train['has_superstructure_cement_mortar_brick'] \n",
    "train['concrete'] = train['has_superstructure_rc_non_engineered'] | train['has_superstructure_rc_engineered'] \n",
    "train['natural'] = train['has_superstructure_timber'] | train['has_superstructure_bamboo'] \n",
    "\n",
    "test_and_train_values['mud'] = test_and_train_values['has_superstructure_adobe_mud'] | test_and_train_values['has_superstructure_mud_mortar_stone'] | test_and_train_values['has_superstructure_mud_mortar_brick']\n",
    "test_and_train_values['cement'] = test_and_train_values['has_superstructure_cement_mortar_stone'] | test_and_train_values['has_superstructure_cement_mortar_brick'] \n",
    "test_and_train_values['concrete'] = test_and_train_values['has_superstructure_rc_non_engineered'] | test_and_train_values['has_superstructure_rc_engineered'] \n",
    "test_and_train_values['natural'] = test_and_train_values['has_superstructure_timber'] | test_and_train_values['has_superstructure_bamboo'] \n",
    "\n",
    "test_values['mud'] = test_values['has_superstructure_adobe_mud'] | test_values['has_superstructure_mud_mortar_stone'] | test_values['has_superstructure_mud_mortar_brick']\n",
    "test_values['cement'] = test_values['has_superstructure_cement_mortar_stone'] | test_values['has_superstructure_cement_mortar_brick'] \n",
    "test_values['concrete'] = test_values['has_superstructure_rc_non_engineered'] | test_values['has_superstructure_rc_engineered'] \n",
    "test_values['natural'] = test_values['has_superstructure_timber'] | test_values['has_superstructure_bamboo'] \n",
    "\n",
    "train['n_struc_types'] = train['mud'] + train['cement'] + train['concrete'] + train['natural']\n",
    "test_and_train_values['n_struc_types'] = test_and_train_values['mud'] + test_and_train_values['cement'] + test_and_train_values['concrete'] + test_and_train_values['natural']\n",
    "test_values['n_struc_types'] = test_values['mud'] + test_values['cement'] + test_values['concrete'] + test_values['natural']\n",
    "\n",
    "train['concrete_only'] = (train['concrete']==True) & (train['n_struc_types']==1)\n",
    "train['cement_only'] = (train['cement']==True) & (train['n_struc_types']==1)\n",
    "\n",
    "test_and_train_values['concrete_only'] = (test_and_train_values['concrete']==True) & (test_and_train_values['n_struc_types']==1)\n",
    "test_and_train_values['cement_only'] = (test_and_train_values['cement']==True) & (test_and_train_values['n_struc_types']==1)\n",
    "\n",
    "test_values['concrete_only'] = (test_values['concrete']==True) & (test_values['n_struc_types']==1)\n",
    "test_values['cement_only'] = (test_values['cement']==True) & (test_values['n_struc_types']==1)\n",
    "\n",
    "# This next one is just a helper column for use later to get averages of damage for each region\n",
    "train['no-mud'] = abs(train['mud']-1)\n",
    "test_and_train_values['no-mud'] = abs(test_and_train_values['mud']-1)\n",
    "test_values['no-mud'] = abs(test_values['mud']-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_only = (train['concrete']==True) & (train['n_struc_types']==1)\n",
    "mud_only = (train['mud']==True) & (train['n_struc_types']==1)\n",
    "cement_only = (train['cement']==True) & (train['n_struc_types']==1)\n",
    "natural_only = (train['natural']==True) & (train['n_struc_types']==1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations about the building types:\n",
    "* If building has some mud, damage is at least 2\n",
    "* The distribution of damage isn't materially different whether the building is all mud or just has some mud.\n",
    "* All-concrete buildings suffer very little damage\n",
    "* All-cement are also strong, but not quite as good as all-concrete\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train[train['mud']==True].damage_grade\n",
    "             .value_counts()\n",
    "             .sort_index()\n",
    "             .plot.bar(title=\"Number of mud Buildings with Each Damage Grade\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train[mud_only].damage_grade\n",
    "             .value_counts()\n",
    "             .sort_index()\n",
    "             .plot.bar(title=\"Number of mud only Buildings with Each Damage Grade\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train[train['mud']==False].damage_grade\n",
    "             .value_counts()\n",
    "             .sort_index()\n",
    "             .plot.bar(title=\"Number of non-mud Buildings with Each Damage Grade\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train[train['natural']==True].damage_grade\n",
    "             .value_counts()\n",
    "             .sort_index()\n",
    "             .plot.bar(title=\"Number of natural Buildings with Each Damage Grade\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train[natural_only].damage_grade\n",
    "             .value_counts()\n",
    "             .sort_index()\n",
    "             .plot.bar(title=\"Number of natural only Buildings with Each Damage Grade\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train[train['cement']==True].damage_grade\n",
    "             .value_counts()\n",
    "             .sort_index()\n",
    "             .plot.bar(title=\"Number of cement Buildings with Each Damage Grade\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train[cement_only].damage_grade\n",
    "             .value_counts()\n",
    "             .sort_index()\n",
    "             .plot.bar(title=\"Number of cement only Buildings with Each Damage Grade\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train[(train['concrete']==True)].damage_grade\n",
    "             .value_counts()\n",
    "             .sort_index()\n",
    "             .plot.bar(title=\"Number of concrete Buildings with Each Damage Grade\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(train[concrete_only].damage_grade\n",
    "             .value_counts()\n",
    "             .sort_index()\n",
    "             .plot.bar(title=\"Number of concrete only Buildings with Each Damage Grade\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struc_types = ['mud','natural','cement','concrete']\n",
    "combs = []\n",
    "for i in range(len(struc_types)):\n",
    "    combs.extend([list(t) for t in [k for k in itertools.combinations(struc_types,i+1)]])\n",
    "    \n",
    "combs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = [train[t]==True for t in combs[12]]\n",
    "train[np.logical_and.reduce(filters)].damage_grade.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.logical_and.reduce(filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in combs:\n",
    "    filters = [train[t]==True for t in c]\n",
    "    print(c)\n",
    "    print(train[np.logical_and.reduce(filters)].damage_grade\n",
    "             .mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[list(t) for t in [k for k in itertools.combinations(struc_types,2)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some geolocations might only be in the test set, so if we are going to build a universal lookup then we need to include test as well so we can get a complete list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_and_train_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_and_train.head()\n",
    "test_and_train_values.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_lookup = test_and_train_values[['geo_level_3_id','geo_level_2_id','geo_level_1_id']].groupby(['geo_level_3_id']).first().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "geo_lookup.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OK, now we can calcualte averages for each geoid level and construction type, and build our lookup table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I'm going to double check what the frequency is like for the different structure types, because I'm worried that we won't have many examples of concrete and if that is an issue then we need to account for it somehow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = ['1','2','3']\n",
    "structure_cats = ['mud','no-mud','cement_only', 'concrete_only']\n",
    "\n",
    "\n",
    "# plot\n",
    "f, axes = plt.subplots(1, 4, figsize=(14, 4), sharex=True, sharey=True)\n",
    "\n",
    "# Change the x-axis because it has a really long tail.  First attempt was to make it log, second just trims\n",
    "#axes[0,0].set(xscale=\"log\")\n",
    "axes[0].set_xlim(right=20)\n",
    "\n",
    "graph_colours = ['skyblue','olive', 'gold', 'teal']\n",
    "ax = [axes[0],axes[1],axes[2],axes[3]]\n",
    "\n",
    "for i,s in enumerate(structure_cats):\n",
    "    sns.distplot( geo_lookup[s+'1_n'].fillna(0) , color=graph_colours[i], ax=ax[i], bins=300)\n",
    "\n",
    "#################################\n",
    "f, axes = plt.subplots(1, 4, figsize=(14, 4), sharex=True, sharey=True)\n",
    "\n",
    "# Change the x-axis because it has a really long tail.  First attempt was to make it log, second just trims\n",
    "#axes[0,0].set(xscale=\"log\")\n",
    "axes[0].set_xlim(right=20)\n",
    "\n",
    "graph_colours = ['skyblue','olive', 'gold', 'teal']\n",
    "ax = [axes[0],axes[1],axes[2],axes[3]]\n",
    "\n",
    "for i,s in enumerate(structure_cats):\n",
    "    sns.distplot( geo_lookup[s+'2_n'].fillna(0) , color=graph_colours[i], ax=ax[i], bins=300)\n",
    "\n",
    "\n",
    "#################################\n",
    "f, axes = plt.subplots(2, 2, figsize=(7, 7), sharex=True, sharey=True)\n",
    "\n",
    "# Change the x-axis because it has a really long tail.  First attempt was to make it log, second just trims\n",
    "#axes[0,0].set(xscale=\"log\")\n",
    "axes[0,0].set_xlim(right=20)\n",
    "\n",
    "graph_colours = ['skyblue','olive', 'gold', 'teal']\n",
    "ax = [axes[0, 0],axes[0, 1],axes[1, 0],axes[1, 1]]\n",
    "\n",
    "\n",
    "for i,s in enumerate(structure_cats):\n",
    "    sns.distplot( geo_lookup[s+'3_n'].fillna(0) , color=graph_colours[i], ax=ax[i], bins=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure_cats = ['mud','natural', 'cement', 'concrete']\n",
    "\n",
    "for location in biggest_geo3:\n",
    "    print('Geo3 id:',location)\n",
    "    for s in structure_cats:\n",
    "        s_filter = (train['geo_level_3_id'] == location) & (train[s] == 1)\n",
    "        print(s, train.loc[s_filter]['damage_grade'].count(), train.loc[s_filter]['damage_grade'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages = {}\n",
    "levels = ['1','2','3']\n",
    "for level in levels:\n",
    "    for s in structure_cats:\n",
    "        s_filter = train[s] == 1\n",
    "        averages[s+level] = train[s_filter].groupby('geo_level_'+level+'_id')['damage_grade'].agg({s+level+'_n':'count', \n",
    "                                     s+level+'_mean':'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages['mud1'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.merge(averages['mud2'].reset_index(), how='left',on='geo_level_2_id').head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Everything is looking OK, let's do it again but this time merge inline rather than saving to a dictionary first\n",
    "We use train here because that df has got the damage values in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = ['1','2','3']\n",
    "structure_cats = ['mud','no-mud','cement_only', 'concrete_only']\n",
    "\n",
    "\n",
    "for level in levels:\n",
    "    print('averaging level',level)\n",
    "    averages_list = []\n",
    "\n",
    "    # Work out normalised damage grades for each structure type\n",
    "    for s in structure_cats:\n",
    "        s_filter = train[s] == 1\n",
    "        averages = train[s_filter].groupby('geo_level_'+level+'_id')['damage_grade'].agg({s+level+'_n':'count', \n",
    "                                     s+level+'_mean':'mean'})\n",
    "        col_to_norm = averages[s+level+'_mean']\n",
    "        averages[s+level+'_mean_norm']=(col_to_norm-col_to_norm.min())/(col_to_norm.max()-col_to_norm.min())\n",
    "        #print(averages.head(2))\n",
    "        averages_list.append(averages)\n",
    "\n",
    "    # Concat the averages into one dataframe\n",
    "    averages = pd.concat(averages_list, axis=1)\n",
    "    #print(averages.head())\n",
    "    #print(geo_lookup.shape)\n",
    "    \n",
    "    # Now we have those, we can also calculate a weighted average across the structure types for that geoid\n",
    "    \n",
    "    cols = [s+level+'_mean_norm' for s in structure_cats]\n",
    "    weights = [s+level+'_n' for s in structure_cats]\n",
    "\n",
    "    norms_np = averages[cols].values\n",
    "    weights_np = averages[weights].values\n",
    "\n",
    "    norm_mask = np.isnan(norms_np)\n",
    "    weights_mask = np.isnan(weights_np)\n",
    "\n",
    "    norms_np = np.ma.masked_array(norms_np, mask=norm_mask)\n",
    "    weights_np = np.ma.masked_array(weights_np, mask=weights_mask)\n",
    "\n",
    "    wa_norm = np.ma.average(norms_np, weights=weights_np, axis=1)\n",
    "    wa_norm.fill_value = -1\n",
    "    averages['level'+level+'norm_damage'] = wa_norm.filled()\n",
    "    #geo_lookup['level'+level+'_wa_norm_damage'] = wa_norm.filled()\n",
    "    #print(averages.head())\n",
    "    \n",
    "    # Add on the columns we want to keep after all of that\n",
    "    geo_lookup = geo_lookup.merge(averages['level'+level+'norm_damage'].reset_index(), how='left',on='geo_level_'+level+'_id')\n",
    "    geo_lookup = geo_lookup.merge(averages['mud'+level+'_mean_norm'].reset_index(), how='left',on='geo_level_'+level+'_id')\n",
    "    geo_lookup = geo_lookup.merge(averages['no-mud'+level+'_mean_norm'].reset_index(), how='left',on='geo_level_'+level+'_id')\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some  geoids have missing values though, presumably because the only examples are in the training set.  So we can use the next available level up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_lookup.loc[8313]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_level = geo_lookup['level2norm_damage'].isnull()\n",
    "geo_lookup.loc[empty_level,'level2norm_damage'] = geo_lookup.loc[empty_level,'level1norm_damage']\n",
    "\n",
    "empty_level = geo_lookup['level3norm_damage'].isnull()\n",
    "geo_lookup.loc[empty_level,'level3norm_damage'] = geo_lookup.loc[empty_level,'level2norm_damage']\n",
    "\n",
    "\n",
    "empty_level = geo_lookup['mud2_mean_norm'].isnull()\n",
    "geo_lookup.loc[empty_level,'mud2_mean_norm'] = geo_lookup.loc[empty_level,'mud1_mean_norm']\n",
    "\n",
    "empty_level = geo_lookup['mud3_mean_norm'].isnull()\n",
    "geo_lookup.loc[empty_level,'mud3_mean_norm'] = geo_lookup.loc[empty_level,'mud2_mean_norm']\n",
    "\n",
    "\n",
    "empty_level = geo_lookup['no-mud2_mean_norm'].isnull()\n",
    "geo_lookup.loc[empty_level,'no-mud2_mean_norm'] = geo_lookup.loc[empty_level,'no-mud1_mean_norm']\n",
    "\n",
    "empty_level = geo_lookup['no-mud3_mean_norm'].isnull()\n",
    "geo_lookup.loc[empty_level,'no-mud3_mean_norm'] = geo_lookup.loc[empty_level,'no-mud2_mean_norm']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_lookup.loc[8313]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, join the lookup table with the test/train values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_and_train_values = test_and_train_values.merge(geo_lookup[['geo_level_3_id','level3norm_damage','mud3_mean_norm','no-mud3_mean_norm']], on=['geo_level_3_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_and_train_values[22011:22015]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the categorical into numeric\n",
    "\n",
    "This code taken from https://stackoverflow.com/questions/24458645/label-encoding-across-multiple-columns-in-scikit-learn\n",
    "\n",
    "categorical_encoder = defaultdict(LabelEncoder)\n",
    "\n",
    "'# Encoding the variable\n",
    "\n",
    "fit = df.apply(lambda x: categorical_encoder[x.name].fit_transform(x))\n",
    "\n",
    "'# Inverse the encoded\n",
    "\n",
    "fit.apply(lambda x: categorical_encoder[x.name].inverse_transform(x))\n",
    "\n",
    "'# Using the dictionary to label future data\n",
    "\n",
    "df.apply(lambda x: categorical_encoder[x.name].transform(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\n",
    "    'land_surface_condition',\n",
    "    'foundation_type',\n",
    "    'roof_type',\n",
    "    'ground_floor_type',\n",
    "    'other_floor_type',\n",
    "    'position',\n",
    "    'plan_configuration',\n",
    "    'legal_ownership_status'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_columns:\n",
    "    test_and_train_values[col] = pd.Categorical(test_and_train_values[col])\n",
    "    dfDummies = pd.get_dummies(test_and_train_values[col], prefix = col)\n",
    "    test_and_train_values = pd.concat([test_and_train_values, dfDummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_columns:\n",
    "    train[col] = pd.Categorical(train[col])\n",
    "    dfDummies = pd.get_dummies(train[col], prefix = col)\n",
    "    train = pd.concat([train, dfDummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_columns:\n",
    "    train_values[col] = pd.Categorical(train_values[col])\n",
    "    dfDummies = pd.get_dummies(train_values[col], prefix = col)\n",
    "    train_values = pd.concat([train_values, dfDummies], axis=1)\n",
    "    \n",
    "    test_values[col] = pd.Categorical(test_values[col])\n",
    "    dfDummies = pd.get_dummies(test_values[col], prefix = col)\n",
    "    test_values = pd.concat([test_values, dfDummies], axis=1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(categorical_columns, axis=1)\n",
    "test_and_train_values = test_and_train_values.drop(categorical_columns, axis=1)\n",
    "train_values = train_values.drop(categorical_columns, axis=1)\n",
    "test_values = test_values.drop(categorical_columns, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(geo_lookup[['geo_level_3_id','level3norm_damage','mud3_mean_norm','no-mud3_mean_norm']], on=['geo_level_3_id'], how='left')\n",
    "test_values = test_values.merge(geo_lookup[['geo_level_3_id','level3norm_damage','mud3_mean_norm','no-mud3_mean_norm']], on=['geo_level_3_id'], how='left')\n",
    "test_and_train_values = test_and_train_values.merge(geo_lookup[['geo_level_3_id','level3norm_damage','mud3_mean_norm','no-mud3_mean_norm']], on=['geo_level_3_id'], how='left')\n",
    "train_values = train_values.merge(geo_lookup[['geo_level_3_id','level3norm_damage','mud3_mean_norm','no-mud3_mean_norm']], on=['geo_level_3_id'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_encoder = defaultdict(LabelEncoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the encoder on the combined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_and_train_values.loc[:,categorical_columns] = test_and_train_values.loc[:,categorical_columns].apply(lambda x: categorical_encoder[x.name].fit_transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_and_train_values.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the encoder to transform the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_values.loc[:,categorical_columns] = train_values.loc[:,categorical_columns].apply(lambda x: categorical_encoder[x.name].transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All features are now categorical so we could use them on a classifier now, but...let's engineer some features first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(geo_lookup[['geo_level_3_id','level3norm_damage','mud3_mean_norm','no-mud3_mean_norm']], on=['geo_level_3_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[:,categorical_columns] = train.loc[:,categorical_columns].apply(lambda x: categorical_encoder[x.name].transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep the test data too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_values = test_values.merge(geo_lookup[['geo_level_3_id','level3norm_damage','mud3_mean_norm','no-mud3_mean_norm']], on=['geo_level_3_id'], how='left')\n",
    "test_values.loc[:,categorical_columns] = test_values.loc[:,categorical_columns].apply(lambda x: categorical_encoder[x.name].transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_values.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Damage measures for each geolocation\n",
    "\n",
    "The idae here is that I want a normalised measure of average damage per geolocation.  The easiest way would be to take the average damage value for the geoid but that wouldn't take into consideration the different mix of building types.  If some geolocations had sturdier buildings then it's average damage might be artifically low.\n",
    "\n",
    "So instead I'll take the average damage for each building type and/or some sort of adjustment for the building type - for example wooden buildings seem to have less damage so perhaps we can work out some normalised values for each building type and then combine them to get a single normalised damage value for each geoid.\n",
    "\n",
    "The last factor I want to allow for is that some geoids have only one data point, and that isn't gong to be of much use, so instead my intial approach will be to take the average of the next geolevel up if the count of datapoints is below a certain threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ex_geo = train.drop(['geo_level_1_id','geo_level_2_id','geo_level_3_id', 'damage_grade'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_ex_geo, train['damage_grade'], test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=500, max_depth=15)#, class_weight='balanced')\n",
    "%time clf.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(list(zip(X_train.columns,clf.feature_importances_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "f1_score(y_test, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train on full dataset and predict probabilities so we can combine predictions across models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time clf.fit(train_ex_geo, train['damage_grade']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select only a few of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_use = [\n",
    "    'level3norm_damage',\n",
    "    'mud3_mean_norm',\n",
    "    'no-mud3_mean_norm',\n",
    "    'mud',\n",
    "    'cement_only',\n",
    "    'concrete_only',\n",
    "    'count_floors_pre_eq',\n",
    "    'age',\n",
    "    'foundation_type',\n",
    "    'roof_type',\n",
    "    'area_percentage',\n",
    "    'height_percentage',\n",
    "    'ground_floor_type'\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_col = []\n",
    "for col_prefix in cols_to_use:\n",
    "    filter_col.append([col for col in df if col.startswith(col_prefix)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_select_cols = train[cols_to_use]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_select_cols, train['damage_grade'], test_size=0.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=500, max_depth=15, class_weight='balanced')\n",
    "%time clf.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(list(zip(X_train.columns,clf.feature_importances_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "f1_score(y_test, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = RandomForestClassifier(n_estimators=500, max_depth=15)#, class_weight='balanced')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time clf1.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "f1_score(y_test, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train on full data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time clf1.fit(train_select_cols, train['damage_grade']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=500, max_depth=15, class_weight={1:1.5,2:1,3:1.2})\n",
    "%time clf.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "f1_score(y_test, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfLD = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')\n",
    "%time clfLD.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predLD = clfLD.predict(X_test)\n",
    "f1_score(y_test, y_predLD, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNeighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_k = KNeighborsClassifier(15)\n",
    "%time clf_k.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_k = clf_k.predict(X_test)\n",
    "f1_score(y_test, y_pred_k, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OK, now let's try that with XGBoost on the smaller dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train-1)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify parameters via map\n",
    "param = {'max_depth':6, 'eta':0.3, 'subsample':1, 'objective':'multi:softmax', 'num_class':3 }\n",
    "num_round = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time bst = xgb.train(param, dtrain, num_round)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction\n",
    "y_predXG = bst.predict(dtest) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, y_predXG, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain_full = xgb.DMatrix(train_ex_geo, label=train['damage_grade']-1) \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to softprob so that we get the probabilities, which allows us to ensemble with the other models\n",
    "param = {'max_depth':6, 'eta':0.3, 'subsample':1, 'objective':'multi:softprob', 'num_class':3 }\n",
    "%time clfXGB = xgb.train(param, dtrain_full, num_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predXG[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predLD[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And XGBoost on all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_ex_geo, train['damage_grade'], test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train-1)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify parameters via map\n",
    "param = {'max_depth':10, 'eta':0.3, 'subsample':1, 'objective':'multi:softmax', 'num_class':3 }\n",
    "num_round = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time bst = xgb.train(param, dtrain, num_round)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction\n",
    "y_predXG = bst.predict(dtest) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, y_predXG, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_ex_geo, train['damage_grade'], test_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label=y_train-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst = xgb.train(param, dtrain, num_round)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Also try imbalance XGBoost\n",
    "Can't get past the errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imxgboost.imbalance_xgb import imbalance_xgboost as imb_xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import LeaveOneOut , cross_validate\n",
    "\n",
    "# focal XGBoost\n",
    "xgboster_focal = imb_xgb(special_objective='focal')\n",
    "# cross - validation booster\n",
    "CV_focal_booster = GridSearchCV ( xgboster_focal , {\"focal_gamma\":[1.0 , 2.0 , 3.0]})\n",
    "# fit the booster\n",
    "CV_focal_booster . fit (X_train.values , y_train.values )\n",
    "# retrieve the best model and parameter\n",
    "opt_focal_booster = CV_focal_booster . best_estimator_\n",
    "opt_focal_parameter = CV_focal_booster . best_params_\n",
    "# instantialize an imbalance - xgboost instance for cross - validation\n",
    "xgboost_focal_opt = imb_xgb (special_objective='focal', ** xgboost_opt_param )\n",
    "# initialize a leave -one splitter\n",
    "loo_splitter = LeaveOneOut ()\n",
    "# Leave - One cross validation\n",
    "loo_info_dict = cross_validate ( xgboost_focal_opt , X=X_train , y=y_train-1 , cv= loo_splitter )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try grid search with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_grid = XGBClassifier(learning_rate=0.3, n_estimators=600, objective='multi:softmax',\n",
    "                    silent=False, nthread=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_params = {\n",
    "    'max_depth' : [5,7,10,13],\n",
    "#    'gamma': [0, 0.2],\n",
    "#    'subsample': [0.8, 1],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_micro = make_scorer(f1_score, average='micro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(estimator=xgb_grid, param_grid=grid_params, scoring=f1_micro, n_jobs=-1, cv=2, verbose=10 )\n",
    "grid.fit(X_train, y_train-1)\n",
    "print('\\n All results:')\n",
    "print(grid.cv_results_)\n",
    "print('\\n Best estimator:')\n",
    "print(grid.best_estimator_)\n",
    "print('\\n Best score:')\n",
    "print(grid.best_score_ * 2 - 1)\n",
    "print('\\n Best parameters:')\n",
    "print(grid.best_params_)\n",
    "results = pd.DataFrame(grid.cv_results_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_classifier = XGBClassifier(learning_rate=0.1, n_estimators=600, objective='multi:softmax',\n",
    "                    silent=False, nthread=-1, max_depth=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time xgb_classifier.fit(X_train, y_train-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_XGBC = xgb_classifier.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " f1_score(y_test, y_pred_XGBC+1, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ex_geo = test_values.drop(['geo_level_1_id','geo_level_2_id','geo_level_3_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtest = xgb.DMatrix(test_ex_geo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = bst.predict(dtest) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_format = pd.read_csv(DATA_DIR / 'submission_format.csv', index_col='building_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_submission = pd.DataFrame(data=y_pred,\n",
    "                             columns=submission_format.columns,\n",
    "                             index=submission_format.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_submission.damage_grade = my_submission.damage_grade.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head submission.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Ensemble submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ex_geo = test_values.drop(['geo_level_1_id','geo_level_2_id','geo_level_3_id'], axis=1)\n",
    "#test_small = test_values[cols_to_use]\n",
    "test_xgb = xgb.DMatrix(test_ex_geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf_full = clf.predict_proba(test_ex_geo)   \n",
    "#y_pred_rf_small = clf1.predict_proba(test_small)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xg_small = clfXGB.predict(test_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xg_small[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = np.mean([y_pred_rf_full,y_pred_rf_small,y_pred_xg_small], axis=0)\n",
    "y_pred = np.mean([y_pred_rf_full,y_pred_xg_small], axis=0)\n",
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.argmax(y_pred, axis=1) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_format = pd.read_csv(DATA_DIR / 'submission_format.csv', index_col='building_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_submission = pd.DataFrame(data=pred,\n",
    "                             columns=submission_format.columns,\n",
    "                             index=submission_format.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_submission.damage_grade = my_submission.damage_grade.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
